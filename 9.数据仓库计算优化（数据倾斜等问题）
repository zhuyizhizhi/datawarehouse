
hive函数参考手册  https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF

Hive 优化

核心思想：把Hive SQL 当做Mapreduce程序去优化

常用的设置参数
set hive.exec.dynamic.partition = true;
set hive.exec.dynamic.partition.mode = nonstrict;
set mapreduce.yarn.job.priority=2;

set mapred.job.name =$target.table$now.datekey;
set hive.exec.parallel=true;
set hive.auto.convert.join = true;
##set hive.merge.mapfiles = true;
##set hive.merge.mapredfiles = true;
##set hive.merge.smallfiles.avgsize=80000000;
##set mapreduce.map.java.opts="-Xmx5120m"; 
##set mapreduce.map.memory.mb=8000;

set mapred.reduce.tasks = 600;
set mapred.max.split.size=256000000;
set mapred.min.split.size.per.node=100000000;
set mapred.min.split.size.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set hive.merge.mapfiles = true ;
set hive.merge.mapredfiles = true;
##set hive.merge.size.per.task = 256*1000*1000 ;
set hive.merge.smallfiles.avgsize=64000000 ;
set mapreduce.map.java.opts="-Xmx5120m"; 
set mapreduce.map.memory.mb=8000;
set mapred.reduce.tasks = -1;


1MAP/REDUCE参数设置
    1.1调节MAP个数
    1.2调节REDUCE个数
2小文件问题
3数据倾斜
      1、Count(distinct)引发数据倾斜问题
      2、空的数据，或者大量同样的数据，参与关联，或者参与partition by，轻则影响性能，重则内存溢出报错
      3、不同数据类型id的关联会产生数据倾斜问题
      4、Mapjoin解决数据倾斜
      5、自己动手让数据均衡分布解决数据倾斜
      6、数据倾斜解决问题思路总结
      4合理使用union all和multi insert
5order by、sort by、distribute by和cluster by
      1. order by
      2. sort by
      3. distribute by
      4. cluster by
      
1MAP/REDUCE参数设置
1.1调节MAP个数
inputFormat这个类是用来处理Map的输入数据的，任务开始时，InputFormat先将HDFS里所有输入文件里的数据分割成逻辑上的InputSpilt对象
这里的split是HDFS中block的部分或者一整块或几个块中的数据的逻辑分割，一个split对应于一个Map，所以Map的数量是由split的数量决定的。
那么怎样去确定InputSpilt的个数呢，下面列出于split个数相关的配置参数：
numSplits：来自job.getNumMapTasks()，即在job启动时用org.apache.hadoop.mapred.JobConf.setNumMapTasks(int n)设置的值，给M-R框架的Map数量的提示。
minSplitSize：默认为1B，可由子类复写函数protected void setMinSplitSize(long minSplitSize) 重新设置。一般情况下，都为1，特殊情况除外。
blockSize：HDFS的块大小，默认为64M，一般大的HDFS都设置成128M。
splitSize=max{minSize,min{maxSize,blockSize}}
其中minSize=max{minSplitSize,mapred.min.split.size}，maxSize=mapred.max.split.size
1.    通常情况下，作业会通过input的目录产生一个或者多个map任务。 
主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小(目前为128M, 可在hive中通过set dfs.block.size;命令查看到，该参数不能自定义修改)；
 
2.    举例： 
a)    假设input目录下有1个文件a,大小为780M,那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数
b)    假设input目录下有3个文件a,b,c,大小分别为10m，20m，130m，那么hadoop会分隔成4个块（10m,20m,128m,2m）,从而产生4个map数
即，如果文件大于块大小(128m),那么会拆分，如果小于块大小，则把该文件当成一个块。
 
3.    是不是map数越多越好？ 
       答案是否定的。如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。
 
4.    是不是保证每个map处理接近128m的文件块，就高枕无忧了？ 
       答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。
针对上面的问题3和4，我们需要采取两种方式来解决：即减少map数和增加map数；
1、减少MAP数
— 如果是小文件造成的map数太大，可以参考合并小文件的办法：
-- 每个Map最大输入大小，决定合并后的文件数
set mapred.max.split.size=256000000;
-- 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并
set mapred.min.split.size.per.node=100000000;
-- 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并
set mapred.min.split.size.per.rack=100000000;
-- 执行Map前进行小文件合并
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 
— 如果没有小文件，最简单方法是通过设置mapred.max.split.size来调小map数，将其设置大一点（mapred.max.split.size：每个map的最大输入大小，该值越小，map数越多）
2、增加MAP数
1）如果表a只有一个文件，大小为120M，但包含几千万的记录，如果用1个map去完成这个任务，肯定是比较耗时的，这种情况下，我们要考虑将这一个文件合理的拆分成多个，这样就可以用多个map任务去完成。
                   set mapred.reduce.tasks=10;--调节REDUCE个数
                   create table a_1 as  select * from a distribute by rand(123);
    这样会将a表的记录，随机的分散到包含10个文件的a_1表中，再用a_1代替上面sql中的a表，则会用10个map任务去完成。
    每个map任务处理大于12M（几百万记录）的数据，效率肯定会好很多
2）通过设置mapred.max.split.size来调大map数，也可配合mapred.min.split.size一起来调节，将其设置小一点

1.2调节REDUCE个数
Reduce数如何决定？
参数1：hive.exec.reducers.bytes.per.reducer（每个reduce任务处理的数据量，默认为1G）
参数2 ：hive.exec.reducers.max（每个任务最大的reduce数，默认为999）
—计算reducer数的公式很简单N=min（参数2，总输入数据量/参数1）
通过设置这两个参数来减少或增加reduce数：
set hive.exec.reducers.bytes.per.reducer=256000000;##每个reduce处理的数据大小
set hive.exec.reducers.max=2000;##增加最大reduce个数
set mapred.reduce.tasks=10;##设置reduce个数（这种方法不推荐，因为你也不知道改用多少个reducers）



2小文件问题
1、小文件是如何产生的
-动态分区插入数据，产生大量小文件，导致map数剧增；
-Reduce数越多，小文件也越多；
-数据是直接导入的小文件；
-其他原因
2、根据产生原因可以从源头控制小文件数
-使用Sequence file作为表存储格式，不要用TextFile
-减少Reduce数量（可以使用参数控制）
-少用动态分区，用时记得一定要按distribute by 分区。
 
3、小文件的影响
-从hive角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行浪费大量资源，严重影响性能。
-在HDFS的文件元信息，包括位置、大小、分块信息等，都保存在NAMENODE的内存中，每个小文件对象约占150byte，一千万个文件及分块就会占用约3G的内存，一旦接近这个量级，NAMENODE的性能就开始下降。

4、小文件的解决
最常用的是
set mapred.max.split.size=100000000;  每个map的最大输入大小，该值约小，map数越多
set mapred.min.split.size.per.node=100000000; 
set mapred.min.split.size.per.rack=100000000; 
set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 
hive.input.format=……表示合并小文件。大于文件块大小128m的，按照128m来分隔，小于128m,大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并,最终生成了74个块
建表时为什么会有很多小文件？是因为多次加载每次都放到新文件？还是中间有处理过程，reduce输出时造成小文件过多。


3数据倾斜

表现：

任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。
 
单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。 最长时长远大于平均时长

1、Count(distinct)引发数据倾斜问题
-原因是会按group by字段分布数据，如果group by后面的值很少，比如只有男女，那就只有两台机器执行，distinct后面的字段会排序；
-解决方案1：设置参数：set hive.groupby.skewindata=true; 有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个MR Job（第一个MR Job Map的输出结果随机分配到reduce做次预汇总,减少某些key值条数过多某些key条数过小造成的数据倾斜问题）。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。

-解决方案2：先group by把数据减少，在基础上再进行group by数据量就少点。

2、空的数据，或者大量同样的数据，参与关联，或者参与partition by，轻则影响性能，重则内存溢出报错
—有些特定场景，比如未登陆用户的user_id访问情况可能会填一个空或者0的值；
-这些数据参与关联或者partition by会都分配到一台机器上做处理，导致出现问题。
-解决方法1：参数解决，skewjoin
-解决方法2：过滤掉这些不用的数据再关联
-解决方法3：手动让其重新分布on(case when a.user_id is null then concat(‘xxx’,rand()) else a.user_id end)=b.user_id;

3、不同数据类型id的关联会产生数据倾斜问题
-一定要确保两个关联的字段是相同类型
-比如string和bigint关联，string会被转成bigint，有些转不了的全部放到一台机器上了，所以出现问题。为什么hive不是往string转，原因：BUG。
解决方法：全部转成string再关联
-其他不同类型的关联也可能出现这种情况。

4、Mapjoin解决数据倾斜
小表关联时，MAPJION会把小表全部读入内存中，在map阶段直接拿另外一个表的数据和内存中表数据做匹配，这样就不会由于数据倾斜导致某个reduce上落数据太多而失败。由于在map是进行了join操作，省去了reduce运行的效率也会高很多
在适合使用mapjoin的场景，可以在select后使用 /*+ mapjoin(a)*/的方式，或者设置自动开启mapjoin模式参数，set hive.auto.convert.join=true;
select /*+ mapjoin(A)*/ f.a,f.b from A t join B f  on ( f.a=t.a and f.ftime=20110802) 
mapjoin一般要求小表小于100M，如果小表过大，超过mapjoin适合的场景。比如member表100万条记录，日志log表上亿条记录，就不能简单的使用mapjoin了。但通过了解到业务场景，每天活跃的用户数memberid比较少，  则可以先对log表的member_id去重后，使用mapjoin关联member表，然后再和log表通过mapjoin关联。

5、自己动手让数据均衡分布解决数据倾斜
   A表与B表关联，没有复杂逻辑，只是通过id去关联。A表每天10多亿条记录，B表约三四亿记录，两表直接关联出现数据倾斜，执行一天数据近2个小时。
   对表进行数据均衡处理，分别增加一个字段r，A表的r是0-99随机分布，B关联0-99的数组，使其数据翻100倍，在关联时用上r字段，使A表数据被打散。
6、数据倾斜解决问题思路总结
-分层次逐步减少数据量；
-过滤掉倾斜的数据；
不能过滤就想办法让其均衡分布；
-不能均衡分布就想办法让倾斜的数据做mapjoin;
-如果还不能解决就试一下倾斜参数设置。
hive.groupby.skewindata=true：数据倾斜时负载均衡，当选项设定为true，生成的查询计划会有两个MRJob。第一个MRJob 中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupBy Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照GroupBy Key分布到Reduce中（这个过程可以保证相同的GroupBy Key被分布到同一个Reduce中），最后完成最终的聚合操作。
