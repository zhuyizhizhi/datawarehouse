
hive函数参考手册  https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF

Hive 优化

核心思想：把Hive SQL 当做Mapreduce程序去优化

常用的设置参数
set hive.exec.dynamic.partition = true;
set hive.exec.dynamic.partition.mode = nonstrict;
set mapreduce.yarn.job.priority=2;

set mapred.job.name =$target.table$now.datekey;
set hive.exec.parallel=true;
set hive.auto.convert.join = true;
##set hive.merge.mapfiles = true;
##set hive.merge.mapredfiles = true;
##set hive.merge.smallfiles.avgsize=80000000;
##set mapreduce.map.java.opts="-Xmx5120m"; 
##set mapreduce.map.memory.mb=8000;

set mapred.reduce.tasks = 600;
set mapred.max.split.size=256000000;
set mapred.min.split.size.per.node=100000000;
set mapred.min.split.size.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set hive.merge.mapfiles = true ;
set hive.merge.mapredfiles = true;
##set hive.merge.size.per.task = 256*1000*1000 ;
set hive.merge.smallfiles.avgsize=64000000 ;
set mapreduce.map.java.opts="-Xmx5120m"; 
set mapreduce.map.memory.mb=8000;
set mapred.reduce.tasks = -1;


1MAP/REDUCE参数设置
    1.1调节MAP个数
    1.2调节REDUCE个数
2小文件问题
3数据倾斜
      1、Count(distinct)引发数据倾斜问题
      2、空的数据，或者大量同样的数据，参与关联，或者参与partition by，轻则影响性能，重则内存溢出报错
      3、不同数据类型id的关联会产生数据倾斜问题
      4、Mapjoin解决数据倾斜
      5、自己动手让数据均衡分布解决数据倾斜
      6、数据倾斜解决问题思路总结
      4合理使用union all和multi insert
5order by、sort by、distribute by和cluster by
      1. order by
      2. sort by
      3. distribute by
      4. cluster by
      
1MAP/REDUCE参数设置
1.1调节MAP个数
inputFormat这个类是用来处理Map的输入数据的，任务开始时，InputFormat先将HDFS里所有输入文件里的数据分割成逻辑上的InputSpilt对象
这里的split是HDFS中block的部分或者一整块或几个块中的数据的逻辑分割，一个split对应于一个Map，所以Map的数量是由split的数量决定的。
那么怎样去确定InputSpilt的个数呢，下面列出于split个数相关的配置参数：
numSplits：来自job.getNumMapTasks()，即在job启动时用org.apache.hadoop.mapred.JobConf.setNumMapTasks(int n)设置的值，给M-R框架的Map数量的提示。
minSplitSize：默认为1B，可由子类复写函数protected void setMinSplitSize(long minSplitSize) 重新设置。一般情况下，都为1，特殊情况除外。
blockSize：HDFS的块大小，默认为64M，一般大的HDFS都设置成128M。
splitSize=max{minSize,min{maxSize,blockSize}}
其中minSize=max{minSplitSize,mapred.min.split.size}，maxSize=mapred.max.split.size
1.    通常情况下，作业会通过input的目录产生一个或者多个map任务。 
主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小(目前为128M, 可在hive中通过set dfs.block.size;命令查看到，该参数不能自定义修改)；
 
2.    举例： 
a)    假设input目录下有1个文件a,大小为780M,那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数
b)    假设input目录下有3个文件a,b,c,大小分别为10m，20m，130m，那么hadoop会分隔成4个块（10m,20m,128m,2m）,从而产生4个map数
即，如果文件大于块大小(128m),那么会拆分，如果小于块大小，则把该文件当成一个块。
 
3.    是不是map数越多越好？ 
       答案是否定的。如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。
 
4.    是不是保证每个map处理接近128m的文件块，就高枕无忧了？ 
       答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。
针对上面的问题3和4，我们需要采取两种方式来解决：即减少map数和增加map数；
1、减少MAP数
— 如果是小文件造成的map数太大，可以参考合并小文件的办法：
-- 每个Map最大输入大小，决定合并后的文件数
set mapred.max.split.size=256000000;
-- 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并
set mapred.min.split.size.per.node=100000000;
-- 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并
set mapred.min.split.size.per.rack=100000000;
-- 执行Map前进行小文件合并
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 
— 如果没有小文件，最简单方法是通过设置mapred.max.split.size来调小map数，将其设置大一点（mapred.max.split.size：每个map的最大输入大小，该值越小，map数越多）
2、增加MAP数
1）如果表a只有一个文件，大小为120M，但包含几千万的记录，如果用1个map去完成这个任务，肯定是比较耗时的，这种情况下，我们要考虑将这一个文件合理的拆分成多个，这样就可以用多个map任务去完成。
                   set mapred.reduce.tasks=10;--调节REDUCE个数
                   create table a_1 as  select * from a distribute by rand(123);
    这样会将a表的记录，随机的分散到包含10个文件的a_1表中，再用a_1代替上面sql中的a表，则会用10个map任务去完成。
    每个map任务处理大于12M（几百万记录）的数据，效率肯定会好很多
2）通过设置mapred.max.split.size来调大map数，也可配合mapred.min.split.size一起来调节，将其设置小一点

1.2调节REDUCE个数
Reduce数如何决定？
参数1：hive.exec.reducers.bytes.per.reducer（每个reduce任务处理的数据量，默认为1G）
参数2 ：hive.exec.reducers.max（每个任务最大的reduce数，默认为999）
—计算reducer数的公式很简单N=min（参数2，总输入数据量/参数1）
通过设置这两个参数来减少或增加reduce数：
set hive.exec.reducers.bytes.per.reducer=256000000;##每个reduce处理的数据大小
set hive.exec.reducers.max=2000;##增加最大reduce个数
set mapred.reduce.tasks=10;##设置reduce个数（这种方法不推荐，因为你也不知道改用多少个reducers）



2小文件问题
1、小文件是如何产生的
-动态分区插入数据，产生大量小文件，导致map数剧增；
-Reduce数越多，小文件也越多；
-数据是直接导入的小文件；
-其他原因
2、根据产生原因可以从源头控制小文件数
-使用Sequence file作为表存储格式，不要用TextFile
-减少Reduce数量（可以使用参数控制）
-少用动态分区，用时记得一定要按distribute by 分区。
 
3、小文件的影响
-从hive角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行浪费大量资源，严重影响性能。
-在HDFS的文件元信息，包括位置、大小、分块信息等，都保存在NAMENODE的内存中，每个小文件对象约占150byte，一千万个文件及分块就会占用约3G的内存，一旦接近这个量级，NAMENODE的性能就开始下降。

4、小文件的解决
最常用的是
set mapred.max.split.size=100000000;  每个map的最大输入大小，该值约小，map数越多
set mapred.min.split.size.per.node=100000000; 
set mapred.min.split.size.per.rack=100000000; 
set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 
hive.input.format=……表示合并小文件。大于文件块大小128m的，按照128m来分隔，小于128m,大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并,最终生成了74个块
建表时为什么会有很多小文件？是因为多次加载每次都放到新文件？还是中间有处理过程，reduce输出时造成小文件过多。


3数据倾斜

表现：

任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。
 
单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。 最长时长远大于平均时长

