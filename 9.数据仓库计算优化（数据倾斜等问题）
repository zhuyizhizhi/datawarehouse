
hive函数参考手册  https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF

Hive 优化

核心思想：把Hive SQL 当做Mapreduce程序去优化

常用的设置参数
set hive.exec.dynamic.partition = true;
set hive.exec.dynamic.partition.mode = nonstrict;
set mapreduce.yarn.job.priority=2;

set mapred.job.name =$target.table$now.datekey;
set hive.exec.parallel=true;
set hive.auto.convert.join = true;
##set hive.merge.mapfiles = true;
##set hive.merge.mapredfiles = true;
##set hive.merge.smallfiles.avgsize=80000000;
##set mapreduce.map.java.opts="-Xmx5120m"; 
##set mapreduce.map.memory.mb=8000;

set mapred.reduce.tasks = 600;
set mapred.max.split.size=256000000;
set mapred.min.split.size.per.node=100000000;
set mapred.min.split.size.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set hive.merge.mapfiles = true ;
set hive.merge.mapredfiles = true;
##set hive.merge.size.per.task = 256*1000*1000 ;
set hive.merge.smallfiles.avgsize=64000000 ;
set mapreduce.map.java.opts="-Xmx5120m"; 
set mapreduce.map.memory.mb=8000;
set mapred.reduce.tasks = -1;


1MAP/REDUCE参数设置
    1.1调节MAP个数
    1.2调节REDUCE个数
2小文件问题
3数据倾斜
      1、Count(distinct)引发数据倾斜问题
      2、空的数据，或者大量同样的数据，参与关联，或者参与partition by，轻则影响性能，重则内存溢出报错
      3、不同数据类型id的关联会产生数据倾斜问题
      4、Mapjoin解决数据倾斜
      5、自己动手让数据均衡分布解决数据倾斜
      6、数据倾斜解决问题思路总结
      4合理使用union all和multi insert
5order by、sort by、distribute by和cluster by
      1. order by
      2. sort by
      3. distribute by
      4. cluster by
      
1MAP/REDUCE参数设置
1.1调节MAP个数
inputFormat这个类是用来处理Map的输入数据的，任务开始时，InputFormat先将HDFS里所有输入文件里的数据分割成逻辑上的InputSpilt对象
这里的split是HDFS中block的部分或者一整块或几个块中的数据的逻辑分割，一个split对应于一个Map，所以Map的数量是由split的数量决定的。
那么怎样去确定InputSpilt的个数呢，下面列出于split个数相关的配置参数：
numSplits：来自job.getNumMapTasks()，即在job启动时用org.apache.hadoop.mapred.JobConf.setNumMapTasks(int n)设置的值，给M-R框架的Map数量的提示。
minSplitSize：默认为1B，可由子类复写函数protected void setMinSplitSize(long minSplitSize) 重新设置。一般情况下，都为1，特殊情况除外。
blockSize：HDFS的块大小，默认为64M，一般大的HDFS都设置成128M。
splitSize=max{minSize,min{maxSize,blockSize}}
其中minSize=max{minSplitSize,mapred.min.split.size}，maxSize=mapred.max.split.size
1.    通常情况下，作业会通过input的目录产生一个或者多个map任务。 
主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小(目前为128M, 可在hive中通过set dfs.block.size;命令查看到，该参数不能自定义修改)；
 
2.    举例： 
a)    假设input目录下有1个文件a,大小为780M,那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数
b)    假设input目录下有3个文件a,b,c,大小分别为10m，20m，130m，那么hadoop会分隔成4个块（10m,20m,128m,2m）,从而产生4个map数
即，如果文件大于块大小(128m),那么会拆分，如果小于块大小，则把该文件当成一个块。
 
3.    是不是map数越多越好？ 
       答案是否定的。如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。
 
4.    是不是保证每个map处理接近128m的文件块，就高枕无忧了？ 
       答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。
针对上面的问题3和4，我们需要采取两种方式来解决：即减少map数和增加map数；
1、减少MAP数
— 如果是小文件造成的map数太大，可以参考合并小文件的办法：
-- 每个Map最大输入大小，决定合并后的文件数
set mapred.max.split.size=256000000;
-- 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并
set mapred.min.split.size.per.node=100000000;
-- 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并
set mapred.min.split.size.per.rack=100000000;
-- 执行Map前进行小文件合并
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 
— 如果没有小文件，最简单方法是通过设置mapred.max.split.size来调小map数，将其设置大一点（mapred.max.split.size：每个map的最大输入大小，该值越小，map数越多）
2、增加MAP数
1）如果表a只有一个文件，大小为120M，但包含几千万的记录，如果用1个map去完成这个任务，肯定是比较耗时的，这种情况下，我们要考虑将这一个文件合理的拆分成多个，这样就可以用多个map任务去完成。
                   set mapred.reduce.tasks=10;--调节REDUCE个数
                   create table a_1 as  select * from a distribute by rand(123);
    这样会将a表的记录，随机的分散到包含10个文件的a_1表中，再用a_1代替上面sql中的a表，则会用10个map任务去完成。
    每个map任务处理大于12M（几百万记录）的数据，效率肯定会好很多
2）通过设置mapred.max.split.size来调大map数，也可配合mapred.min.split.size一起来调节，将其设置小一点

1.2调节REDUCE个数
Reduce数如何决定？
参数1：hive.exec.reducers.bytes.per.reducer（每个reduce任务处理的数据量，默认为1G）
参数2 ：hive.exec.reducers.max（每个任务最大的reduce数，默认为999）
—计算reducer数的公式很简单N=min（参数2，总输入数据量/参数1）
通过设置这两个参数来减少或增加reduce数：
set hive.exec.reducers.bytes.per.reducer=256000000;##每个reduce处理的数据大小
set hive.exec.reducers.max=2000;##增加最大reduce个数
set mapred.reduce.tasks=10;##设置reduce个数（这种方法不推荐，因为你也不知道改用多少个reducers）



2小文件问题
1、小文件是如何产生的
-动态分区插入数据，产生大量小文件，导致map数剧增；
-Reduce数越多，小文件也越多；
-数据是直接导入的小文件；
-其他原因
2、根据产生原因可以从源头控制小文件数
-使用Sequence file作为表存储格式，不要用TextFile
-减少Reduce数量（可以使用参数控制）
-少用动态分区，用时记得一定要按distribute by 分区。
 
3、小文件的影响
-从hive角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行浪费大量资源，严重影响性能。
-在HDFS的文件元信息，包括位置、大小、分块信息等，都保存在NAMENODE的内存中，每个小文件对象约占150byte，一千万个文件及分块就会占用约3G的内存，一旦接近这个量级，NAMENODE的性能就开始下降。

4、小文件的解决
最常用的是
set mapred.max.split.size=100000000;  每个map的最大输入大小，该值约小，map数越多
set mapred.min.split.size.per.node=100000000; 
set mapred.min.split.size.per.rack=100000000; 
set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 
hive.input.format=……表示合并小文件。大于文件块大小128m的，按照128m来分隔，小于128m,大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并,最终生成了74个块
建表时为什么会有很多小文件？是因为多次加载每次都放到新文件？还是中间有处理过程，reduce输出时造成小文件过多。


3数据倾斜

表现：

任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。
 
单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。 最长时长远大于平均时长

1、Count(distinct)引发数据倾斜问题
-原因是会按group by字段分布数据，如果group by后面的值很少，比如只有男女，那就只有两台机器执行，distinct后面的字段会排序；
-解决方案1：设置参数：set hive.groupby.skewindata=true; 有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个MR Job（第一个MR Job Map的输出结果随机分配到reduce做次预汇总,减少某些key值条数过多某些key条数过小造成的数据倾斜问题）。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。

-解决方案2：先group by把数据减少，在基础上再进行group by数据量就少点。

2、空的数据，或者大量同样的数据，参与关联，或者参与partition by，轻则影响性能，重则内存溢出报错
—有些特定场景，比如未登陆用户的user_id访问情况可能会填一个空或者0的值；
-这些数据参与关联或者partition by会都分配到一台机器上做处理，导致出现问题。
-解决方法1：参数解决，skewjoin
-解决方法2：过滤掉这些不用的数据再关联
-解决方法3：手动让其重新分布on(case when a.user_id is null then concat(‘xxx’,rand()) else a.user_id end)=b.user_id;

3、不同数据类型id的关联会产生数据倾斜问题
-一定要确保两个关联的字段是相同类型
-比如string和bigint关联，string会被转成bigint，有些转不了的全部放到一台机器上了，所以出现问题。为什么hive不是往string转，原因：BUG。
解决方法：全部转成string再关联
-其他不同类型的关联也可能出现这种情况。

4、Mapjoin解决数据倾斜
小表关联时，MAPJION会把小表全部读入内存中，在map阶段直接拿另外一个表的数据和内存中表数据做匹配，这样就不会由于数据倾斜导致某个reduce上落数据太多而失败。由于在map是进行了join操作，省去了reduce运行的效率也会高很多
在适合使用mapjoin的场景，可以在select后使用 /*+ mapjoin(a)*/的方式，或者设置自动开启mapjoin模式参数，set hive.auto.convert.join=true;
select /*+ mapjoin(A)*/ f.a,f.b from A t join B f  on ( f.a=t.a and f.ftime=20110802) 
mapjoin一般要求小表小于100M，如果小表过大，超过mapjoin适合的场景。比如member表100万条记录，日志log表上亿条记录，就不能简单的使用mapjoin了。但通过了解到业务场景，每天活跃的用户数memberid比较少，  则可以先对log表的member_id去重后，使用mapjoin关联member表，然后再和log表通过mapjoin关联。

5、自己动手让数据均衡分布解决数据倾斜
   A表与B表关联，没有复杂逻辑，只是通过id去关联。A表每天10多亿条记录，B表约三四亿记录，两表直接关联出现数据倾斜，执行一天数据近2个小时。
   对表进行数据均衡处理，分别增加一个字段r，A表的r是0-99随机分布，B关联0-99的数组，使其数据翻100倍，在关联时用上r字段，使A表数据被打散。
6、数据倾斜解决问题思路总结
-分层次逐步减少数据量；
-过滤掉倾斜的数据；
不能过滤就想办法让其均衡分布；
-不能均衡分布就想办法让倾斜的数据做mapjoin;
-如果还不能解决就试一下倾斜参数设置。
hive.groupby.skewindata=true：数据倾斜时负载均衡，当选项设定为true，生成的查询计划会有两个MRJob。第一个MRJob 中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupBy Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照GroupBy Key分布到Reduce中（这个过程可以保证相同的GroupBy Key被分布到同一个Reduce中），最后完成最终的聚合操作。



4合理使用union all和multi insert
 
    multi insert适合基于同一个源表按照不同逻辑不同粒度处理插入不同表的场景，做到只需要扫描源表一次，job个数不变，减少源表扫描次数
    union all用好，可减少表的扫描次数，减少job的个数,通常预先按不同逻辑不同条件生成的查询union all后，再统一group by计算,不同表的union all相当于multiple inputs,同一个表的union all,相当map一次输出多条
 
对同一张表的union all 要比多重insert快的多，原因是hive本身对这种union all做过优化（只局限于非嵌套查询），即只扫描一次源表；
而多重insert也只扫描一次，但因为要insert到多个分区，所以做了很多其他的事情，导致消耗的时间非常长；
不同字段先union再关联：
比如推广效果表要和商品表关联，效果表中的auction id列既有商品id,也有数字id,和商品表关联得到商品的信息。那么以下的hive sql性能会比较好
Select * from effect a
Join (select auction_id as auction_id from auctions
Union all
Select auction_string_id as auction_id from auctions
) b
On a.auction_id = b.auction_id。
比分别过滤数字id,字符串id然后分别和商品表关联性能要好。
这样写的好处,1个MR作业,商品表只读取一次，推广效果表只读取一次。把这个sql换成MR代码的话，map的时候，把a表的记录打上标签a,商品表记录每读取一条，打上标签b，变成两个<key ,value>对，<b,数字id>，<b,字符串id>。所以商品表的hdfs读只会是一次。
先join生成临时表，在union all还是写嵌套查询，这是个问题。比如以下例子：
Select *
From (select *
     From t1
     Uion all
     select *
     From t4
     Union all
     Select *
     From t2
     Join t3
     On t2.id = t3.id
     ) x
Group by c1,c2;
这个会有4个jobs。假如先join生成临时表的话t5,然后union all，会变成2个jobs。
Insert overwrite table t5
Select *
     From t2
     Join t3
     On t2.id = t3.id
;
Select * from (t1 union all t4 union all t5) ;
hive在union all优化上可以做得更智能（把子查询当做临时表），这样可以减少开发人员的负担。出现这个问题的原因应该是union all目前的优化只局限于非嵌套查询。如果写MR程序这一点也不是问题，就是multi inputs。


5order by、sort by、distribute by和cluster by
1. order by
    Hive中的order by跟传统的sql语言中的order by作用是一样的，会对查询的结果做一次全局排序，所以说，只有hive的sql中制定了order by所有的数据都会到同一个reducer进行处理（不管有多少map，也不管文件有多少的block只会启动一个reducer）。但是对于大量数据这将会消耗很长的时间去执行。
    这里跟传统的sql还有一点区别：如果指定了hive.mapred.mode=strict（默认值是nonstrict）,这时就必须指定limit来限制输出条数，原因是：所有的数据都会在同一个reducer端进行，数据量大的情况下可能不能出结果，那么在这样的严格模式下，必须指定输出的条数。
2. sort by
    Hive中指定了sort by，那么在每个reducer端都会做排序，也就是说保证了局部有序（每个reducer出来的数据是有序的，但是不能保证所有的数据是有序的，除非只有一个reducer），好处是：执行了局部排序之后可以为接下去的全局排序提高不少的效率（其实就是做一次归并排序就可以做到全局排序了）。
    
    3. distribute by
 
hive中的distribute by是控制在map端如何拆分数据给reduce端的。
hive会根据distribute by后面列，根据reduce的个数进行数据分发，默认是采用hash算法。
对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。
4. cluster by
    cluster by的功能就是distribute by和sort by相结合，如下2个语句是等价的：
    
select mid, money, name from store cluster by mid  
select mid, money, name from store distribute by mid sort by mid  
    如果需要获得与3中语句一样的效果：
select mid, money, name from store cluster by mid sort by money  
    注意被cluster by指定的列只能是降序，不能指定asc和desc。
